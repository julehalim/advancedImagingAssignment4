%%
%% This is file `./samples/longsample.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% apa7.dtx  (with options: `longsample')
%% ----------------------------------------------------------------------
%% 
%% apa7 - A LaTeX class for formatting documents in compliance with the
%% American Psychological Association's Publication Manual, 7th edition
%% 
%% Copyright (C) 2021 by Daniel A. Weiss <daniel.weiss.led at gmail.com>
%% 
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%% 
%% http://www.latex-project.org/lppl.txt
%% 
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%% 
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%% 
%% ----------------------------------------------------------------------
%% 
\documentclass[man]{apa7}

\usepackage{lipsum}

\usepackage[american]{babel}

\usepackage{caption} % For captioning outside figures
\usepackage{capt-of} % For captioning outside figures

\usepackage{siunitx}

\usepackage{csquotes}
\usepackage[style=apa,backend=biber]{biblatex}
\addbibresource{bibliography.bib}

\title{Parking Occupancy Detection from CCTV Images}
\shorttitle{}
\authorsnames{Jule Valendo Halim -1425567}
\authorsaffiliations{GEOM90038 - Advanced Imaging}

\begin{document}

\maketitle
\tableofcontents
\newpage
\section{Introduction}

Recent developments in data has undergone significant changes from data processing into machine learning due to increased volumes of readily available data (\textcite{KHAN20201444}). The introduction of image-based neural network architecture has allowed the field of computer vision to flourish. Computer vision leverages the power of machine learning to derive meaningful information from visual data, which allows them to take actions and recommendations when they detect issues. Powerful advances in machine learning such as the widely popular transformer model that Chat-GPT is based on has also been adapted to be trained on visual information (\textcite{elnouby2021training}). Figure \ref{fig:representationNeuralNetwork} provides a representation of a neural network, which consists of multiple layers. The input layer is the initial data, and hidden layers calculate the weights of each of these data and propagates them forward to other hidden layers. Finally, the output layer provides probabilities of the desired output (e.g., some classification label).

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/4 ,width=\textwidth/1]{figures/sampleNeuralNetwork.png}
  \captionof{figure}{Visual Representation of Layers That Makes Up a Neural Network (\textcite{neuralNetwork})}
  \label{fig:representationNeuralNetwork}
\end{minipage}

Figure \ref{fig:exampleArchitecture} provides a simplified neural network architecture for visual data. The visual data that is placed into the input layer depends on the specific architecture and design. In the case of figure \ref{fig:exampleArchitecture}, a section of the image is placed into the input layer. It is then propagated forward towards additional layers before finally obtaining the results of the output layer, which in this case, attempts to classify the image into four possible species.

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/4 ,width=\textwidth/1]{figures/exampleML.jpg}
  \captionof{figure}{Sample Architecture of Machine Learning Being Used in Computer Vision (\textcite{CHAI2021100134})}
  \label{fig:exampleArchitecture}
\end{minipage}

The use of computer vision has been applied to wide range of different fields. For example, \textcite{MEDICAL} discussed the way medical imaging applications in multiple medical areas such as pathology and dermatology has enhanced the level of care provided. Another field in which machine learning has enhanced computer vision is in the automation and digitization of fruit quality measuring and maintainence (\textcite{SUPERMARKET}). Figure \ref{fig:exampleDetection} shows how computer vision can be used to detect cars and humans in images. These detections are generated post-training, where multiple images are provided for training the neural network. The trained neural network can then be given new images or even real time video to detect what they were trained to. However, as seen in figure \ref{fig:exampleDetection}, such neural networks are not perfect, and can misclassify or not detect parts of the image that they are supposed to (e.g., the model did not successfully classify one of the humans walking).

\newpage

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/4 ,width=\textwidth/2]{figures/detectionExample.png}
  \captionof{figure}{Sample Image Detection Results of Cars and People Using Deep Learning and RCNN Models(\textcite{KHAN20201444})}
  \label{fig:exampleDetection}
\end{minipage}

Faster Recurrent Neural Network models (FasterRCNN model) are a form of neural network that can detect objects in images. The FasterRCNN model is based off \textcite{fasterRCNN}. It consists of two main components, the Region Proposal Network (RPN) and the FastRCNN. RPNs are a convolutional neural network, which are a form of neural network such as those described above, but are specialized in taking three dimensional data (such as images) as inputs. RPNs are further specialized to handle different schemas of different images. Figure \ref{fig:RPN} shows the different schemas that the RPN can handle, namely differing scales, differing filter sizes, and multiple references of the same image. 

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/4 ,width=\textwidth/2]{figures/detectionExample.png}
  \captionof{figure}{Different Schemas that RPNs Are Adapted To(\textcite{fasterRCNN})}
  \label{fig:RPN}
\end{minipage}

\newpage

The other component, the FastRCNN (\textcite{fastRCNN}) is a neural network that has been shown to be much faster than other image detection neural networks. The main innovation that causes this increased detection speed is the use of of Regions of Interest (RoI). The RoI is defined with a four-tuple (r,c,h,w). The r and c specifies the top-left corner, while the h and w defines its height and width respectively. These tuples are then max-pooled to convert features inside regions of interest into a small feature map with fixed spatial size (e.g., height and width of 6 x 7). Figure \ref{fig:ROI} shows the overall architecture of the fastRCNN. As seen in this figure, the image is projected into an RoI format before being pooled and trained on a neural network.

\begin{minipage}{\linewidth}
  \includegraphics[]{figures/ROI.png}
  \captionof{figure}{The FastRCNN Architecture, Including the Conversion of Images into ROI(\textcite{fastRCNN})}
  \label{fig:ROI}
\end{minipage}

The FasterRCNN architecture has layers that provide output towards the RPN. This is then used to create a proposal, which basically proposes what sort of schema the image is using (see figure \ref{fig:RPN}). The image that is passed through the RPN is also used to create feature maps, which create is then passed to the the RoI pooling before being passed to a classifier. In this architecture, the RPN is performing the 'attention' task of FasterRCNN. Attention allows the model to incorporate sequential information into the model (e.g., in a picture of a man on a horse, the model is able to remember that the section of the image with a hat is in the upper-middle area of the whole image). Figure \ref{fig:fasterRCNNArchitecture} shows the entirety of the FasterRCNN architecture.

\newpage

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/4 ,width=\textwidth/2]{figures/fasterRCNN2.png}
  \captionof{figure}{The FasterRCNN Architecture, Showing the Components of RPN and FastRCNN RoI (\textcite{fasterRCNN})}
  \label{fig:fasterRCNNArchitecture}
\end{minipage}

The other model used in this report is the Resnet50 CNN model (\textcite{resnet50}). The Resnet50 CNN model's building block consists of a series of weight layers that have an relu activation function (an activation function converts the weights provided by the layer mapped to an output). Relu takes the maximum of two values and returns an output depending on the input (e.g., a relu that is max(0,x) and has a binary classification task to classify 0 and 1 would classify an output as 1 if x is greater than 0, and classify it as 0 if it is less than 0). Figure \ref{fig:resnet50} shows the building block layer of the Resnet50. The Resnet50 CNN consists of multiple of these layers along with other kinds of layers, one of which is the max-pool layer. This increased depth of representation is vital for visual recognition tasks, and this model is popular in many visual recognition tasks.

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/6 ,width=\textwidth/4]{figures/resnet.png}
  \captionof{figure}{The Resnet50 CNN Building Block (Layer)(\textcite{resnet50})}
  \label{fig:resnet50}
\end{minipage}

\newpage

In this report, I aim to use and investigate a pre-trained FasterRCNN model and Resnet50 CNN model(trained on the PKPlot dataset) to detect cars and delineate parking spaces in a picture of the Barry Street parking lot. The software used will be MATLAB 2020a, along with additional toolboxes such as the deep learning toolbox. The methodology of this begins with a visualization of the dataset, before using a pre-trained Resnet50 CNN classifier to identify whether a parking slot is occupied or empty. Afterwards, parking slot delineation is done using FasterRCNN, which detects cars. The dataset for this step involves images of the parking slot and bounding boxes, which identifies the space in which an object is identified (see figure \ref{fig:exampleDetection}, where cars and human areas and identified using bounding boxes).

These results will then be evaluated by calculating recall, precision and accuracy. Afterwards, the object performance will be post-processed to improve the resulting classification. Visual inspection of the resulting bounding boxes will also be compared to the ground truth of where bounding boxes should be. Additional information is found in the methods and results section.

\newpage

\section{Methods and Results}

\subsection{Visualizing Dataset}
This step involves loading in the PKPlot and Barry Street annotated images. These annotations are bounding boxes that are manually set. Figure \ref{fig:datasetImages} shows the annotated images of the PKPlot (left) and Barry Street (right) datasets. The PKPlot dataset consists of over 695,000+ parking space images. For this report, only a segment of the total dataset was used.

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/4,width=\textwidth/2]{figures/pkplot.png}
  \includegraphics[height=\textheight/4,width=\textwidth/2]{figures/barrystreet.png}
  \captionof{figure}{Images of Annotated from the PKPlot Dataset (left) and Barry Street Dataset (right)}
  \label{fig:datasetImages}
\end{minipage}

Figure \ref{fig:pkplot2} shows a sample empty slot and occupied slot found in the annotated PKPlot datasets.

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/4,width=\textwidth/2]{figures/pkplot2.png}
  \captionof{figure}{Sample Image of Empty (left) and Occupied (right) Slots of PKPlot, Sampled From 1500 Empty Spaces and 1500 Occupied Spaces}
  \label{fig:pkplot2}
\end{minipage}


\subsection{Creating Car Detector}

\subsubsection{Re-Training the Resnet50 CNN Model on PKPlot Dataset}
To create the car detector, the pre-trained Resnet50 CNN was used. The model was set to train on 70\% of the dataset, and 30\% was kept as the validation set. The classication layer of the model with a different classification layer that predicts two classes, occupied and empty parking spaces. An additional augmented dataset was also created, which involves two changes, rotating and changing the scales of the original image. This would allow the model to be trained on a larger dataset as well as being able to generalize to unseen car images (e.g., if the same car was rotated, the model can still identify it). The model was then re-trained with the new models. Figure \ref{fig:modelTraining} shows the loss and accuracy of the model over 20 epochs. The loss and accuracy converge at around 5-8 epochs, with the model obtaining near perfect accuracy and nearly no loss on both the validation and test data.

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/4,width=\textwidth/2]{figures/carDetector.png}
  \captionof{figure}{Loss (up) and Accuracy (down) on Validation and Training Datasets, Re-Trained Model on PKPlot}
  \label{fig:modelTraining}
\end{minipage}

\subsubsection{Testing the Model on Barry Street Dataset}
The same model was then used to identify the same labels (occupied and empty parking spaces) on the Barry Street dataset. Figure \ref{fig:confMatrix} shows the confusion matrix of the re-trained model on the Barry Street dataset. It was able to correctly identify the empty and occupied spaces with high accuracy, correctly identifying 99.2\% of the parking spaces (20.4\% were correctly identified as empty and 78.8\% were correctly identified as occupied). 0.8\% of the predictions were incorrect. The model appears to be able to predict occupied areas more accurately than empty areas (100\% accuracy in classifying occupied targets and 96.5\% accuracy is classifying empty spaces).

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/4,width=\textwidth/2]{figures/confMatrix.png}
  \captionof{figure}{Confusion Matrix of Re-Trained Resnet50 CNN Model on Barry Street Data}
  \label{fig:confMatrix}
\end{minipage}

Figure \ref{fig:wrongPred} shows the visualizations of the 21 incorrectly predicted areas, along with their target scores. A target score of greater than 0.5 for each label indicates that it classifies that space as that label (e.g., an occupied score of 0.6763 means that the model predicted that the model is 67.63\% confident that the area is occupied, and the reverse is also true for empty scores). As seen from the figure, occupied scores are generally much higher, meaning that the model was very confident in their wrong predictions of an area being occupied, compared to empty scores, which only has a empty score of 0.58, which does not show high confidence in that prediction.

\newpage

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/2,width=\textwidth/1]{figures/wrongPred.png}
  \captionof{figure}{Visualization of 21 (0.8\% of the total dataset) wrong predictions, along with their target scores}
  \label{fig:wrongPred}
\end{minipage}

\subsection{Automatic Delineation of Parking Spaces}

\subsubsection{Re-Training the FasterRCNN on PKPlot}

This section uses the FasterRCNN pre-trained model that was trained on detection of cars in highways from a mounted camera on a car. It has been shown to be able to predict cars in highways well, but on still CCTV images, the performance is questionable. Similar to the re-training done on the Restnet50 CNN, the FasterRCNN is re-trained on the PKPlot dataset to identify cars and parking spaces. The initial model's performance is shown in figure \ref{fig:modelIterations}. The graphs plot the model performance over iterations. The first graph (from top left) shows the training loss, which is close to 0. The second and third graphs show the results of the RPN layer, both accuracy and RMSE. The last two graphs show the overall model's accuracy and RMSE.

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/2,width=\textwidth/1]{figures/modelIteration.png}
  \captionof{figure}{Results of Re-Training FasterRCNN on PKPlot}
  \label{fig:modelIterations}
\end{minipage}

\subsubsection{Visualizing Re-Trained Model Predictions}

To visualize the prediction results of the model, the predicted areas in which the model classifies as having a car is shown in figure \ref{fig:bStreetPredictions}. The yellow bounding boxes have the confidence scores as well (similar to the scores in \ref{fig:wrongPred}) The prediction confidence are generally quite high, with most correctly identified parking spaces having a confidence of above 0.9, with one exception of the car at the bottom right. However, this car is also cut off and so could contribute to the reduced confidence. Nevertheless, the model was not able to identify all the cars, even in areas where the cars are fully in frame. The car was also unable to correctly identify any empty parking spaces in the image. 

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/2,width=\textwidth/1]{figures/modelPredictions.png}
  \captionof{figure}{FasterRCNN Predictions on the Barry Street}
  \label{fig:bStreetPredictions}
\end{minipage}

\subsubsection{Improving Model Prediction Accuracy}

To improve the accuracy of the model to correctly identify more cars, the model was further trained on more frames from the Barry Street CCTV. The predicted bounding boxes on all the frames were clustered and drawn. Figure \ref{fig:clusterBox} shows the clusters represented as scatter points and bounding boxes.

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/4,width=\textwidth/2]{figures/clusterDot.png}
  \includegraphics[height=\textheight/4,width=\textwidth/2]{figures/clusterBox.png}
  \captionof{figure}{Predicted bounding box clusters of all frames, shown as scatter points (left) and boxes (right)}
  \label{fig:clusterBox}
\end{minipage}

The bounding boxes (figure \ref{fig:clusterBox} (right)) then had their classification scores averaged and plotted. Figure \ref{fig:clusterAvg} shows the resulting averaged cluster and the corresponding classification scores.

\begin{minipage}{\linewidth}
  \includegraphics[height=\textheight/2,width=\textwidth/1]{figures/clusterAvg.png}
  \captionof{figure}{Averaged Bounding Boxes and the Corresponding Classification Scores}
  \label{fig:clusterAvg}
\end{minipage}

\subsection{Evaluation}

To perform evaluation on 

\subsection{Post-Processing Improvements}

\section{Discussion}

\subsection{Accuracy, Precision, and Recall Evaluation}

\subsection{Improvement Based on Assumptions}

\subsection{Challenges and Shortcomings}

\subsection{Scopes of Improvement}

\section{Conclusions and Future Directions}

\section{Appendix}

\printbibliography


\end{document}


















% Theoretical overlap is a parameter that is used in fine registration to indicate how many percent of the points are expected to overlap from one another. Multiple values ranging from 10-100\% were tested. However, 30\% was selected as the final value in order to reduce the final RMS.

% The RMS values show great accuracy, with the coarse registration having an RMS of 1.13cm while the fine registration has 2.27cm of final RMS values.


% \begin{minipage}{\linewidth}
%   \small
%   \captionof{table}{Table Showing Statistical Information About C2C Distances with MLS as the Reference Point Cloud}
%   \setlength{\tabcolsep}{3pt} % reduce column spacing
%   \renewcommand{\arraystretch}{0.8} % reduce row spacing
%   \label{tab:RMStable2}
%   \begin{tabular}{@{}llrr@{}}         \toprule
%   \multicolumn{2}{c}{C2C Distance Statistics }        \\ \toprule{}
%   &  MLS as Reference Point Cloud \\ \midrule
%   Mean (m)      & 0.00091 \\
%   Maximum Value (m)       & 1.55\\
%   Minimum Value (m)       & 0  \\ 
%   Max Error (m) & 0.096 \\\bottomrule
%   \end{tabular}
% \end{minipage}

% \vspace{2em}

%  Meanwhile, figure \ref{fig:c2cMLS} shows the visualization of the C2C distances of only overlapping points in both point clouds.

%  \newpage

%  \begin{minipage}{\linewidth}
%   \includegraphics[height=\textheight/2 ,width=\textwidth/1]{figures/MLSasReference.png}
%   \captionof{figure}{C2C Distance Visualization of MLS as the Reference Point Cloud}
%   \label{fig:c2cMLS}
% \end{minipage}

% As seen in figure \ref{fig:c2cMLS} and table \ref{tab:RMStable2}, the areas in which there is only overlap between the TLS and MLS shows very high quality, where there are no outliers involved. This highlights how the TLS is not prone to many outliers. The distances between both point clouds are also on average, about 0.091cm, compared to the C2C distances when outliers were involved, as seen in table \ref{tab:RMStable}, where the means are about 9cm for the area where all distances were considered. This difference could thus be explained by the outliers inflating the means.

% To further investigate these distances, a histogram of the C2C distance was created from the saved text file of the MLS. Figure \ref{fig:c2chistogram} shows the histogram for C2C distances less than or equal to 1 and distances greater than 1. These distances are based on the initial C2C distance calculations in table \ref{tab:RMStable}.

% \begin{minipage}{\linewidth}
%   \includegraphics[height=\textheight/4 ,width=\textwidth/2]{figures/MLSC2CHistogramWithLine.png}
%   \includegraphics[height=\textheight/4 ,width=\textwidth/2]{figures/MLSC2CHistogramWithLine2.png}
%   \captionof{figure}{Probability distribution of distances less than or equal to 1 (left) and distances higher than 1(right). Note that frequency for the left table is multiplied by \num{1e8}}
%   \label{fig:c2chistogram}
% \end{minipage}



% \begin{minipage}{\linewidth}
%   \includegraphics[height=\textheight/4 ,width=\textwidth/2]{figures/grossError.png}
%   \captionof{figure}{Area With Large C2C Distances in the MLS Point Cloud}
%   \label{fig:grossError}
% \end{minipage}


% \end{document}
